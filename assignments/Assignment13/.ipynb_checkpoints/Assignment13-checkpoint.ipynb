{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, Sequence, Set, Mapping, Dict, Callable, Optional,TypeVar\n",
    "from dataclasses import dataclass\n",
    "from operator import itemgetter\n",
    "from rl.distribution import Categorical, Choose, Constant\n",
    "from rl.markov_decision_process import FiniteMarkovDecisionProcess\n",
    "from rl.markov_decision_process import StateActionMapping\n",
    "from rl.markov_decision_process import FinitePolicy\n",
    "from rl.dynamic_programming import value_iteration_result, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SARSA\n",
    "# using interface from our final\n",
    "\n",
    "S = TypeVar('S')\n",
    "A = TypeVar('A')\n",
    "def get_sarsa_vf_and_policy(\n",
    "    states_actions_dict: Mapping[S, Optional[Set[A]]],\n",
    "    sample_func: Callable[[S, A], Tuple[S, float]],\n",
    "    terminals,\n",
    "    epsilon_greedy_action: Callable[[S, Mapping[A, float],float],A],\n",
    "    episodes: int = 10000,\n",
    "    step_size: float = 0.01\n",
    "    ) -> Tuple[V[S], FinitePolicy[S, A]]:\n",
    "    q: Dict[S, Dict[A, float]] = \\\n",
    "        {s: {a: 0. for a in actions} for s, actions in\n",
    "            states_actions_dict.items() if actions is not None}\n",
    "    nt_states: Set[S] = {s for s in q}\n",
    "    uniform_states: Choose[S] = Choose(nt_states)\n",
    "    for episode_num in range(episodes):\n",
    "        epsilon: float = 1.0 / (episode_num + 1)\n",
    "        state: S = uniform_states.sample()\n",
    "        action = epsilon_greedy_action(S, q, epsilon)            \n",
    "        while state not in terminals:\n",
    "            (next_state, reward)=sample_func(state,action)\n",
    "            if next_state in terminals:\n",
    "                break\n",
    "            next_action = epsilon_greedy_action(next_state, q, epsilon)\n",
    "            q[state][action]=q[state][action]+step_size*(reward+q[next_state][next_action]\n",
    "                                                         -q[state][action])\n",
    "            state = next_state\n",
    "            action= next_action            \n",
    "    vf_dict: V[Cell] = {s: max(d.values()) for s, d in q.items()}\n",
    "    policy: FinitePolicy[S, A] = FinitePolicy(\n",
    "        {s: Constant(max(d.items(), key=itemgetter(1))[0])\n",
    "            for s, d in q.items()}\n",
    "        )\n",
    "    return (vf_dict, policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q-learning\n",
    "def get_q_learning_vf_and_policy(\n",
    "    states_actions_dict: Mapping[S, Optional[Set[A]]],\n",
    "    sample_func: Callable[[S, A], Tuple[S, float]],\n",
    "    terminals,\n",
    "    epsilon_greedy_action: Callable[[S, Mapping[A, float],float],A],\n",
    "    episodes: int = 10000,\n",
    "    step_size: float = 0.01,\n",
    "    epsilon: float = 0.1\n",
    ") -> Tuple[V[S], FinitePolicy[S, A]]:\n",
    "    q: Dict[S, Dict[A, float]] = \\\n",
    "        {s: {a: 0. for a in actions} for s, actions in\n",
    "            states_actions_dict.items() if actions is not None}\n",
    "    nt_states: CellSet = {s for s in q}\n",
    "    uniform_states: Choose[S] = Choose(nt_states)\n",
    "    for episode_num in range(episodes):\n",
    "        state: S = uniform_states.sample()\n",
    "        while state not in self.terminals:\n",
    "            action = epsilon_greedy_action(state, q, epsilon)\n",
    "            (next_state, reward)=sample_func(state,action)\n",
    "            if next_state in terminals:\n",
    "                break\n",
    "            q[state][action]=q[state][action]+step_size*(reward\n",
    "                                                         +max(q[next_state].values())\n",
    "                                                         -q[state][action])\n",
    "            state = next_state\n",
    "            \n",
    "    vf_dict: V[S] = {s: max(d.values()) for s, d in q.items()}\n",
    "    policy: FinitePolicy[S, A] = FinitePolicy(\n",
    "        {s: Constant(max(d.items(), key=itemgetter(1))[0])\n",
    "        for s, d in q.items()})\n",
    "    return (vf_dict, policy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
